Не следует путать с Семантический анализ . Латентно-семантический анализ (ЛСА) ( англ. Latent semantic analysis, LSA ) — это метод обработки информации на естественном языке , анализирующий взаимосвязь между библиотекой документов и терминами, в них встречающимися, и выявляющий характерные факторы ( тематики ), присущие всем документам и терминам. В основе метода латентно-семантического анализа лежат принципы факторного анализа , в частности, выявление латентных связей изучаемых явлений или объектов. При классификации / кластеризации документов этот метод используется для извлечения контекстно-зависимых значений лексических единиц при помощи статистической обработки больших корпусов текстов [ 1 ] . Содержание 1 История 2 Описание работы ЛСА 3 Применение 4 Достоинства и недостатки ЛСА 5 Примечания 6 Ссылки История [ править | править код ] ЛСА был запатентован в 1988 году [ 2 ] Scott Deerwester , Susan Dumais , George Furnas , Richard Harshman , Thomas Landauer , Karen Lochbaum и Lynn Streeter . В области информационного поиска данный подход называют латентно-семантическим индексированием (ЛСИ) . Впервые ЛСА был применен для автоматического индексирования текстов, выявления семантической структуры текста и получения псевдодокументов [ 3 ] . Затем этот метод был довольно успешно использован для представления баз знаний [ 4 ] и построения когнитивных моделей [ 5 ] . В последние годы метод ЛСА часто используется для поиска информации ( индексация документов ), классификации документов [ 6 ] , моделях понимания [ 7 ] и других областях, где требуется выявление главных факторов из массива информационных данных. Описание работы ЛСА [ править | править код ] Анимация процесса обнаружения тематик в матрице «документы-слова». Каждый столбец матрицы соответствует документу, каждая строка — слову. Ячейки матрицы содержат веса слов в документах (например, значения TF-IDF), более тёмные оттенки соответствуют более высокому весу. Алгоритм LSA группирует как документы, которые используют похожие слова, так и слова, которые встречаются в похожем наборе документов. Полученные кластеры в матрице используются для обнаружения латентных (скрытых) компонентов в исходных данных, соответствующих определённым тематикам. [ 8 ] ЛСА можно сравнить с простым видом нейросети , состоящей из трех слоев: первый слой содержит множество слов ( термов ), второй — некое множество документов, соответствующих определённым ситуациям, а третий, средний, скрытый слой представляет собой множество узлов с различными весовыми коэффициентами, связывающих первый и второй слои. В качестве исходной информации ЛСА использует матрицу термы-на-документы , описывающую набор данных, используемый для обучения системы. Элементы этой матрицы содержат, как правило, веса, учитывающие частоты использования каждого терма в каждом документе и участие терма во всех документах ( TF-IDF ).
Наиболее распространенный вариант ЛСА основан на использовании разложения матрицы по сингулярным значениям ( SVD — Singular Value Decomposition ). С помощью SVD-разложения любая матрица раскладывается во множество ортогональных матриц, линейная комбинация которых является достаточно точным приближением к исходной матрице. Говоря более формально, согласно теореме о сингулярном разложении [ 9 ] , любая вещественная прямоугольная матрица может быть разложена на произведение трех матриц: A = U S V T {\displaystyle {\begin{matrix}A=USV^{T}\end{matrix}}} , где матрицы U {\displaystyle {\textbf {U}}} и V {\displaystyle {\textbf {V}}} — ортогональные, а S {\displaystyle {\textbf {S}}} — диагональная матрица, значения на диагонали которой называются сингулярными значениями матрицы A {\displaystyle {\textbf {A}}} .
Буква Т в выражении V T {\displaystyle {\textbf {V}}^{T}} означает транспонирование матрицы. Такое разложение обладает замечательной особенностью: если в матрице S {\displaystyle {\textbf {S}}} оставить только k {\displaystyle {\textbf {k}}} наибольших сингулярных значений, а в матрицах U {\displaystyle {\textbf {U}}} и V {\displaystyle {\textbf {V}}} — только соответствующие этим значениям столбцы, то произведение получившихся матриц S {\displaystyle {\textbf {S}}} , U {\displaystyle {\textbf {U}}} и V {\displaystyle {\textbf {V}}} будет наилучшим приближением исходной матрицы A {\displaystyle {\textbf {A}}} к матрице A ^ {\displaystyle {\hat {\textbf {A}}}} ранга k {\displaystyle {\textbf {k}}} : A ^ ≈ A = U S V T {\displaystyle {\begin{matrix}{\hat {A}}\approx A=USV^{T}\end{matrix}}} , Основная идея латентно-семантического анализа состоит в том, что если в качестве матрицы A {\displaystyle {\textbf {A}}} использовалась матрица термы-на-документы , то матрица A ^ {\displaystyle {\hat {\textbf {A}}}} , содержащая только k {\displaystyle {\textbf {k}}} первых линейно независимых компонент A {\displaystyle {\textbf {A}}} , отражает основную структуру различных зависимостей, присутствующих в исходной матрице. Структура зависимостей определяется весовыми функциями термов. Таким образом, каждый терм и документ представляются при помощи векторов в общем пространстве размерности k {\displaystyle {\textbf {k}}} (так называемом пространстве гипотез). Близость между любой комбинацией термов и/или документов легко вычисляется при помощи скалярного произведения векторов. Как правило, выбор k {\displaystyle {\textbf {k}}} зависит от поставленной задачи и подбирается эмпирически. Если выбранное значение k {\displaystyle {\textbf {k}}} слишком велико, то метод теряет свою мощность и приближается по характеристикам к стандартным векторным методам. Слишком маленькое значение k не позволяет улавливать различия между похожими термами или документами. Применение [ править | править код ] Существуют три основных разновидности решения задачи методом ЛСА: сравнение двух термов между собой; сравнение двух документов между собой; сравнение терма и документа. Достоинства и недостатки ЛСА [ править | править код ] Достоинства метода: метод является наилучшим для выявления латентных зависимостей внутри множества документов; метод может быть применен как с обучением, так и без обучения (например, для кластеризации ); используются значения матрицы близости, основанной на частотных характеристиках документов и лексических единиц; частично снимается полисемия и омонимия . Недостатки: Существенным недостатком метода является значительное снижение скорости вычисления при увеличении объёма входных данных (например, при SVD-преобразовании). Как показано в [ 3 ] , скорость вычисления соответствует порядку N 2 ∗ k {\displaystyle {\textbf {N}}^{2*k}} , где N = N d o c + N t e r m {\displaystyle {\textbf {N}}={\textbf {N}}_{doc}+{\textbf {N}}_{term}} — сумма количества документов и термов , k {\displaystyle {\textbf {k}}} — размерность пространства факторов. Вероятностная модель метода не соответствует реальности. Предполагается, что слова и документы имеют Нормальное распределение , хотя ближе к реальности Распределение Пуассона . В связи с этим для практических применений лучше подходит Вероятностный латентно-семантический анализ , основанный на мультиномиальном распределении . Примечания [ править | править код ] ↑ Thomas Landauer , Peter W. Foltz, & Darrell Laham. Introduction to Latent Semantic Analysis (англ.) // Discourse Processes [англ.] : journal. — 1998. — Vol. 25 . — P. 259—284 . — doi : 10.1080/01638539809545028 . Архивировано 24 декабря 2010 года. ↑ U.S. Patent 4,839,853 ↑ 1 2 Scott Deerwester , Susan T. Dumais , George W. Furnas , Thomas K. Landauer , Richard Harshman . Indexing by Latent Semantic Analysis (англ.) // Journal of the American Society for Information Science [англ.] : journal. — 1990. — Vol. 41 , no. 6 . — P. 391—407 . — doi : 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9 . Архивировано 17 июля 2012 года. ↑ Thomas Landauer , Susan T. Dumais . A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge (англ.) // JPsychological Review. : journal. — 1997. — Vol. 104 . — P. 211—240 . Архивировано 14 марта 2012 года. ↑ B. Lemaire , G. Denhière . Cognitive Models based on Latent Semantic Analysis (неопр.) // Tutorial given at the 5th International Conference on Cognitive Modeling (ICCM'2003), Bamberg, Germany, April 9 2003.. — 2003. (недоступная ссылка) ↑ Некрестьянов И. С. Тематико-ориентированные методы информационного поиска / Диссертация на соискание степени к. ф-м.н. СПбГУ, 2000. ↑ Соловьев А. Н. Моделирование процессов понимания речи с использованием латентно-семантического анализа / Диссертация на соискание степени к.ф.н. СПбГУ, 2008. ↑ Архивированная копия (неопр.) . Дата обращения: 1 сентября 2017. Архивировано 1 сентября 2017 года. ↑ Голуб Дж., Ван Лоун Ч. Матричные вычисления. М.: «Мир», 1999. Ссылки [ править | править код ] https://web.archive.org/web/20090131212818/http://www-timc.imag.fr/Benoit.Lemaire/lsa.html - Readings in Latent Semantic Analysis for Cognitive Science and Education. — Сборник статей и ссылок о ЛСА. http://lsa.colorado.edu/ - сайт, посвященный моделированию ЛСА. Обработка естественного языка Общие определения Языковая модель Корпус текстов Речевой корпус Стоп-слова Мешок слов AI-полнота N-грамма Биграммный шифр Триграмма Анализ текста Сегментация текста [англ.] Частеречная разметка Поверхностно-синтаксический анализ Обработка сложных слов [англ.] Извлечение коллокаций [англ.] Стемминг Лемматизация Распознавание именованных сущностей [англ.] Разрешение кореферентности Анализ тональности текста Извлечение концептов [англ.] Синтаксический анализ Разрешение лексической многозначности Извлечение терминологии [англ.] Извлечение информации Идентификация языка Определение регистра [англ.] Реферирование Извлечение предложений [англ.] Генерация реферата Многодокументное реферирование [англ.] Упрощение текста [англ.] Машинный перевод Автоматизированный Гибридный Интерлингвальный [англ.] На основе правил На основе примеров На основе словаря [англ.] На основе трансформации Нейронный Статистический Синхронный Идентификация и сбор данных Распознавание речи Синтез речи Оптическое распознавание символов Генерация текста Тематическая модель Размещение патинко Латентное размещение Дирихле Латентно-семантический анализ Рецензирование Автоматизированная оценка сочинений [англ.] Конкордансер Предиктивный ввод текста Система проверки грамматики [англ.] Система проверки правописания Угадывание синтаксиса [англ.] Интерфейс на естественном языке [англ.] Виртуальный ассистент Виртуальный собеседник Вопросно-ответная система Голосовой интерфейс Интерактивная литература Источник — https://ru.wikipedia.org/w/index.php?title=Латентно-семантический_анализ&oldid=143431355