Сообщение о том, что поисковый робот сервиса Wayback Machine не может сканировать сайт Facebook из-за содержимого файла robots.txt, запрещающего сканирование. Стандарт исключений для роботов — стандарт ограничения доступа роботам к содержимому на http -сервере при помощи текстового файла robots.txt , находящегося в корне сайта (то есть имеющего путь относительно имени сайта /robots.txt ). Действие файла не распространяется на сайты, расположенные на поддоменах . Следование стандарту добровольно. Стандарт был принят консорциумом W3C 30 января 1994 года в списке рассылки robots-request@nexor.co.uk и с тех пор используется большинством известных поисковых машин. Файл robots.txt используется для частичного управления обходом сайта поисковыми роботами . Этот файл состоит из набора инструкций для поисковых машин, при помощи которых можно задать файлы, страницы или каталоги сайта, которые не должны запрашиваться. Содержание 1 Описание структуры 2 Проверка синтаксиса 3 Примеры 4 Нестандартные директивы 5 Расширенный стандарт 6 См. также 7 Примечания 8 Ссылки Описание структуры [ править | править код ] Файл состоит из записей. Записи разделяются одной или более пустых строк (признак конца строки : символы CR , CR+ LF , LF ). Каждая запись содержит непустые строки следующего вида: <поле>:<необязательный пробел><значение><необязательный пробел> где поле — это либо User-agent , либо Disallow . В директиве User-agent указываются роботы, которые должны следовать указанным инструкциям (например, User-agent: Yandex , User-agent: YandexBot , User-agent: * ). Сравнение производится методом простого поиска подстроки. Например, запись Disallow: /about запретит доступ как к разделу http://example.com/about/ , так и к файлу http://example.com/about.php , а запись Disallow: /about/ — только к разделу http://example.com/about/ . Файл может содержать комментарии — часть строки, начинающаяся с символа # . Проверка синтаксиса [ править | править код ] Неправильно составленный robots.txt может привести к отрицательным последствиям. Например, весь сайт может «выпасть» из поискового индекса .
Для проверки синтаксиса и структуры файла robots.txt существует ряд специализированных онлайн-служб: Яндекс. Вебмастер — Анализ robots.txt (рус.) (выполняет проверку синтаксиса и разрешения для каждой отдельной страницы) Google Search Console — Инструмент проверки файла robots.txt (рус.) (позволяет проверить разрешения для каждой отдельной страницы) Примеры [ править | править код ] Запрет доступа всех роботов ко всему сайту: User-agent: *
Disallow: / Запрет доступа определённого робота к каталогу /private/: User-agent: googlebot
Disallow: /private/ Нестандартные директивы [ править | править код ] Allow : имеет действие, обратное директиве Disallow — разрешает доступ к определённой части ресурса. Поддерживается всеми основными поисковиками. В следующем примере разрешается доступ к файлу photo.html, а доступ поисковиков ко всей остальной информации в каталоге /album1/ запрещается. Allow: /album1/photo.html
Disallow: /album1/ Crawl-delay : устанавливает время, которое робот должен выдерживать между загрузкой страниц. Если робот будет загружать страницы слишком часто, это может создать излишнюю нагрузку на сервер. Впрочем, современные поисковые машины по умолчанию задают достаточную задержку в 1-2 секунды. На данный момент эта директива не учитывается Googlebot и YandexBot [ 1 ] . User-agent: *
Crawl-delay: 10 Sitemap : расположение файлов Sitemaps , которые могут показать, что именно нужно проиндексировать поисковому роботу. Sitemap: http://example.com/sitemap.xml Расширенный стандарт [ править | править код ] В 1996 году был предложен расширенный стандарт robots.txt , включающий такие директивы как Request-rate и Visit-time. Например: User-agent: *
Disallow: /downloads/
Request-rate: 1/5         # загружать не более одной страницы за пять секунд
Visit-time: 0600-0845     # загружать страницы только в промежуток с 6 утра до 8:45 по Гринвичу. См. также [ править | править код ] Noindex Favicon.ico Sitemaps Примечания [ править | править код ] ↑ Директива Crawl-delay - Вебмастер. Справка (рус.) . yandex.ru . Дата обращения: 1 августа 2021. Архивировано 1 августа 2021 года. Ссылки [ править | править код ] A Standard for Robot Exclusion (англ.) robotstxt.org.ru — о файле robots.txt и роботах в Рунете по-русски О файлах robots.txt — справка Google Использование robots.txt — помощь Яндекса Использование robots.txt — помощь Mail.Ru Поисковая оптимизация Исключения robots.txt Метатеги nofollow noindex Маркетинг Интернет-маркетинг Партнёрская программа Маркетинг по электронной почте Реклама на экране Интернет-статистика Поисковый маркетинг Поисковый маркетинг Оптимизация сайта под социальные сети (SMO) Маркетинг в социальных сетях (SMM) Управление присутствием личной информации в сети Платное включение в индекс Оплата за клик Поисковая бомба Спам Поисковый спам Пессимизация Автоматический сбор данных Сайты с неоригинальным контентом Линкоферма Дорвеи Клоакинг Ссылки Внешние факторы в поисковой оптимизации Популярность ссылки Обмен ссылками Взаимные ссылки Многолинки Биржа ссылок Бэклинки (ссылки на сайт) Поисковый индекс Прочее Страница приземления Геотаргетинг Поисковая система с ручным отбором результатов Статистика запросов Стоп-слова Подозрительные слова Арбитраж Интернет-трафика Источник — https://ru.wikipedia.org/w/index.php?title=Стандарт_исключений_для_роботов&oldid=135641366